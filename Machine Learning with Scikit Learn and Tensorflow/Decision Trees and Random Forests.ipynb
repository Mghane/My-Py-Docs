{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests\n",
    "### Prepared by: Mehdi Ghane\n",
    "### Date: March 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Node**\n",
    "    * Split for the value of a certain attribute\n",
    "* **Edge**\n",
    "    * Outcome of a split to next node\n",
    "* **Root Node**\n",
    "    * The node that performs the first split\n",
    "* **Leaves**\n",
    "    * Terminal nodes that predict the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy and Information Gain are the mathematical methods of choosing the best split. \n",
    "$$H(S) = -\\sum_{i=1} p_i(S).log_2.p_i(S)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizing the Information Gain is to try to choose a feature that best  split the data.\n",
    "\n",
    "*Information gain = Entropy of parent node - average entropy of childeren nodes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for Decision Trees\n",
    "In order to create decision trees that will generalize to new problems well, we can tune a number of different aspects about the trees. We call the different aspects of a decision tree \"hyperparameters\". These are some of the most important hyperparameters used in decision trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum Depth**\n",
    "The maximum depth of a decision tree is simply the largest possible length between the root to a leaf. A tree of maximum length k can have at most 2^k leaves.\n",
    "\n",
    "**Minimum number of samples to split**\n",
    "A node must have at least min_samples_split samples in order to be large enough to split. If a node has fewer samples than min_samples_split samples, it will not be split, and the splitting process stops. However, min_samples_split doesn't control the minimum size of leaves.\n",
    "\n",
    "**Minimum number of samples per leaf**\n",
    "When splitting a node, one could run into the problem of having 99 samples in one of them, and 1 on the other. This will not take us too far in our process, and would be a waste of resources and time. If we want to avoid this, we can set a minimum for the number of samples we allow on each leaf.\n",
    "\n",
    "**HINT**\n",
    "Large depth very often causes overfitting, since a tree that is too deep, can memorize the data. Small depth can result in a very simple model, which may cause underfitting.\n",
    "Small minimum samples per split may result in a complicated, highly branched tree, which can mean the model has memorized the data, or in other words, overfit. Large minimum samples may result in the tree not having enough flexibility to get built, and may result in underfitting.\n",
    "\n",
    "A model designed to have 100% accuracy on training data is unlikely to generalize well to new data. If you pick very large values for your parameters, the model will fit the training set very well, but may not generalize well. Try to find the smallest possible parameters that do the jobâ€”then the model will be more likely to generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep default values for hyperparameters\n",
    "model_tree = DecisionTreeClassifier(\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree.fit(X_train,y_train)\n",
    "y_train_pred = model_tree.predict(X_train)\n",
    "y_test_pred = model_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_pred))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test,y_test_pred))\n",
    "## Test data results in 0.79 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train,y_train_pred))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_train,y_train_pred))\n",
    "## whereas the train set acheived 0.98 accuracy, a clear indication of overfitting issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving model accuracy\n",
    "\n",
    "It seems leaving hyperparmeters on deafult made overfitting which is a far difference between train and test accuracy. We will apply Grid Searching to find the optimum values for hyperparameters. The only downside is that Grid-searching can be extremely computationally expensive and may take a long time to run. Grid-Search will build a model on each parameters combination. It iterates through every parameter combination and stores a model for each combination. When constructing this class we must provide a dictionary of hyperparameters to evaluate in the param_grid argument. This is a map of the model parameter name and an array of values to attempt.\n",
    "\n",
    "**More Insights**\n",
    "\n",
    "* By default, accuracy is the score that is optimized, but other scores can be specified in the score argument of the GridSearchCV constructor.\n",
    "* By default, the grid search will only take one thread. By setting the n_jobs argument in the GridSearchCV constructor to -1, the process can take use of all cores on our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our criterion dictionary\n",
    "criterion = ['gini', 'entropy']\n",
    "splitter = ['best', 'random']\n",
    "max_depth=[6, 7, 8]\n",
    "min_samples_leaf=[ 4, 5, 6, 8]\n",
    "min_samples_split = [8,9,10,12]\n",
    "param_grid = dict(criterion=criterion,splitter=splitter,\n",
    "                  max_depth=max_depth, min_samples_leaf=min_samples_leaf,\n",
    "                  min_samples_split=min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model_tree, param_grid=param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we found our optimal values of hyperparameters, we repeat the prediction using those values\n",
    "model_tree = DecisionTreeClassifier(criterion='gini', splitter='random',\n",
    "                                   max_depth=6, min_samples_leaf=6,\n",
    "                                   min_samples_split=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree.fit(X_train, y_train)\n",
    "y_train_pred = model_tree.predict(X_train)\n",
    "y_test_pred = model_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train,y_train_pred))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_train,y_train_pred))\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running above code, an accuracy of 85% will be acheived for test set with a quite good resolution (not too small) values of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest** is a method to improve performance of single decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Apply Random forest we will build a **Boost Strap** of the sample trainning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
